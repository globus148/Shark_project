# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–æ—Ç–∞
import os
import asyncio
import logging
from dotenv import load_dotenv
from aiogram import Bot, Dispatcher
from aiogram.types import Message, ReplyKeyboardMarkup, KeyboardButton
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.filters import Command, CommandStart
from aiogram.enums import ParseMode
from aiogram.client.default import DefaultBotProperties
from datetime import datetime
import torch
from transformers import AutoModelForSeq2SeqLM, T5TokenizerFast, T5ForConditionalGeneration, T5Tokenizer, AutoTokenizer
import re
from nltk.tokenize import word_tokenize
import nltk

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('instance/bot.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv('tg_env.env')
TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')

if not TOKEN:
    logger.error("TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ñ–∞–π–ª–µ tg_env.env")
    raise ValueError("TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ñ–∞–π–ª–µ tg_env.env")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
bot = Bot(
    token=TOKEN,
    default=DefaultBotProperties(parse_mode=ParseMode.HTML)
)
storage = MemoryStorage()
dp = Dispatcher(storage=storage)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
paraphraser_model = None
paraphraser_tokenizer = None
headliner_model = None
headliner_tokenizer = None


# –°–æ—Å—Ç–æ—è–Ω–∏—è –±–æ—Ç–∞
class ProcessingStates(StatesGroup):
    waiting_for_text = State()


# –ö–ª–∞–≤–∏–∞—Ç—É—Ä—ã
def get_main_keyboard():
    buttons = [
        [KeyboardButton(text="ü§© –ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è")],
        [KeyboardButton(text="üîÑ –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å")],
        [KeyboardButton(text="‚úèÔ∏è –ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫–∏")],
        [KeyboardButton(text="üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫")]
    ]
    return ReplyKeyboardMarkup(keyboard=buttons, resize_keyboard=True)


def get_cancel_keyboard():
    buttons = [[KeyboardButton(text="‚ùå –û—Ç–º–µ–Ω–∏—Ç—å")]]
    return ReplyKeyboardMarkup(keyboard=buttons, resize_keyboard=True)


# –§—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
def correct_spelling(text, max_length=4000, batch_size=4):
    logger.info(f"–ù–∞—á–∞–ª–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞: {text[:100]}...")
    try:
        MODEL_NAME = 'UrukHan/t5-russian-spell'
        tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)
        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)
        model.eval()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è

        task_prefix = "Spell correct: "

        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            dummy_input = tokenizer("test", return_tensors="pt").to(device)
            with torch.no_grad():
                _ = model.generate(**dummy_input, max_length=10)

        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏
        chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]
        results = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞
            encoded = tokenizer(
                [task_prefix + chunk for chunk in batch],
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            ).to(device)

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω—ã–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            with torch.no_grad():
                predicts = model.generate(
                    **encoded,
                    max_length=max_length,
                    num_beams=3,  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                    early_stopping=True
                )

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            batch_results = tokenizer.batch_decode(
                predicts,
                skip_special_tokens=True
            )
            results.extend(batch_results)

        return " ".join(results)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏: {str(e)}")
        raise

def paraphrase_text(text):
    global paraphraser_model, paraphraser_tokenizer
    logger.info(f"–ù–∞—á–∞–ª–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {text[:100]}...")

    try:
        temp = 1.7
        top_k = 60
        top_p = 0.92

        inputs = paraphraser_tokenizer(
            text,
            return_tensors="pt",
            max_length=4000,
            truncation=True,
            padding=True
        ).to(device)

        with torch.no_grad():
            outputs = paraphraser_model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_length=4000,
                num_beams=5,
                do_sample=True,
                temperature=temp,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=2.5,
                early_stopping=True
            )

        decoded_output = paraphraser_tokenizer.decode(outputs[0], skip_special_tokens=True)
        result = clean_paraphrase_output(decoded_output)
        logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω —Ç–µ–∫—Å—Ç. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result[:100]}...")
        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–∏: {str(e)}")
        raise


def clean_paraphrase_output(text):
    text = re.sub(r'^(–ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π:|–ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É—è:|–ø–æ–¥—Ä–æ–±–Ω–µ–µ:|–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:)\s*', '', text, flags=re.IGNORECASE)
    return text.strip()


def should_generate_headline(text):
    words = word_tokenize(text)
    return len(words) >= 20


def generate_short_headline(text):
    global headliner_model, headliner_tokenizer
    logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞: {text[:100]}...")

    try:
        input_ids = headliner_tokenizer(
            text,
            return_tensors="pt",
            max_length=1000,
            truncation=True
        ).input_ids.to(device)

        output_ids = headliner_model.generate(
            input_ids=input_ids,
            max_length=20,
            min_length=5,
            num_beams=4,
            repetition_penalty=3.0,
            length_penalty=1.0,
            early_stopping=True,
            no_repeat_ngram_size=2
        )

        headline = headliner_tokenizer.decode(output_ids[0], skip_special_tokens=True)
        headline = headline.split(".")[0].strip()
        words = [w for w in headline.split() if len(w) > 2][:7]
        result = " ".join(words).capitalize()
        logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫: {result}")
        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {str(e)}")
        raise


# –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥
@dp.message(CommandStart())
@dp.message(Command("help"))
async def send_welcome(message: Message):
    logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {message.from_user.id} –∑–∞–ø—É—Å—Ç–∏–ª –±–æ—Ç–∞")
    welcome_text = (
        "ü§ñ *–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ TextMasterBot!*\n\n"
        "–Ø –ø–æ–º–æ–≥—É –≤–∞–º –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç:\n"
        "‚Ä¢ *–ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è* ‚Äî —É–ø—Ä–æ—â–∞—é —Å–ª–æ–∂–Ω—ã–π —Ç–µ–∫—Å—Ç\n"
        "‚Ä¢ *–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å* ‚Äî –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É—é —Ç–µ–∫—Å—Ç\n"
        "‚Ä¢ *–ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫–∏* ‚Äî –∏—Å–ø—Ä–∞–≤–ª—è—é –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é –∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫—É\n"
        "‚Ä¢ *–ó–∞–≥–æ–ª–æ–≤–æ–∫* ‚Äî —Å–æ–∑–¥–∞—é –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è —Ç–µ–∫—Å—Ç–∞\n\n"
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ –∫–Ω–æ–ø–∫–æ–π –Ω–∏–∂–µ üëá"
    )
    await message.answer(welcome_text, reply_markup=get_main_keyboard())


@dp.message(lambda message: message.text in [
    "ü§© –ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è",
    "üîÑ –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å",
    "‚úèÔ∏è –ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫–∏",
    "üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫"
])
async def process_action(message: Message, state: FSMContext):
    action_map = {
        "ü§© –ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è": "simplify",
        "üîÑ –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å": "paraphrase",
        "‚úèÔ∏è –ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫–∏": "spellcheck",
        "üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫": "headline"
    }
    action = action_map[message.text]
    logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {message.from_user.id} –≤—ã–±—Ä–∞–ª –¥–µ–π—Å—Ç–≤–∏–µ: {action}")

    await state.set_data({"action": action})
    await message.answer(
        "üìù –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:",
        reply_markup=get_cancel_keyboard()
    )
    await state.set_state(ProcessingStates.waiting_for_text)


@dp.message(ProcessingStates.waiting_for_text, lambda message: message.text == "‚ùå –û—Ç–º–µ–Ω–∏—Ç—å")
async def cancel_processing(message: Message, state: FSMContext):
    logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {message.from_user.id} –æ—Ç–º–µ–Ω–∏–ª –æ–ø–µ—Ä–∞—Ü–∏—é")
    try:
        await state.clear()
        await message.answer(
            "‚ùå –î–µ–π—Å—Ç–≤–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ",
            reply_markup=get_main_keyboard()
        )
        logger.info(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {message.from_user.id}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–º–µ–Ω–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: {str(e)}")
        await message.answer(
            "‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–º–µ–Ω–µ –æ–ø–µ—Ä–∞—Ü–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.",
            reply_markup=get_main_keyboard()
        )


@dp.message(ProcessingStates.waiting_for_text)
async def process_text(message: Message, state: FSMContext):
    user_id = message.from_user.id
    logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")

    try:
        user_data = await state.get_data()
        action = user_data.get("action")
        text = message.text.strip()

        if not text:
            logger.warning(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} –æ—Ç–ø—Ä–∞–≤–∏–ª –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
            await message.answer(
                "‚ùó –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –Ω–µ–ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç!",
                reply_markup=get_cancel_keyboard()
            )
            return

        logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} –æ—Ç–ø—Ä–∞–≤–∏–ª —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ ({action}): {text[:100]}...")

        wait_msg = await message.answer("<b>‚è≥ –û–∂–∏–¥–∞–π—Ç–µ...</b>", parse_mode=ParseMode.HTML)
        logger.info(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ '–û–∂–∏–¥–∞–π—Ç–µ...' –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {user_id}")

        try:
            if action == "spellcheck":
                logger.info(f"–ù–∞—á–∞–ª–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
                result = correct_spelling(text)
                # –†–∞–∑–±–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
                max_message_length = 4096  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram
                if len(result) > max_message_length:
                    chunks = [result[i:i+max_message_length] for i in range(0, len(result), max_message_length)]
                    await wait_msg.delete()  # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ "–û–∂–∏–¥–∞–π—Ç–µ..."
                    for i, chunk in enumerate(chunks, 1):
                        await message.answer(
                            f"<b>‚úÖ –ß–∞—Å—Ç—å {i} –∏–∑ {len(chunks)}:</b>\n\n{chunk}",
                            parse_mode=ParseMode.HTML
                        )
                else:
                    await wait_msg.edit_text(
                        text=f"<b>‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:</b>\n\n{result}",
                        parse_mode=ParseMode.HTML
                    )
            elif action == "paraphrase":
                logger.info(f"–ù–∞—á–∞–ª–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
                result = paraphrase_text(text)
                await wait_msg.edit_text(
                    text=f"<b>‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:</b>\n\n{result}",
                    parse_mode=ParseMode.HTML
                )
            elif action == "simplify":
                logger.info(f"–ù–∞—á–∞–ª–æ —É–ø—Ä–æ—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
                corrected = correct_spelling(text)
                result = paraphrase_text(corrected)
                await wait_msg.edit_text(
                    text=f"<b>‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:</b>\n\n{result}",
                    parse_mode=ParseMode.HTML
                )
            elif action == "headline":
                logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
                if should_generate_headline(text):
                    result = generate_short_headline(text)
                    await wait_msg.edit_text(
                        text=f"<b>‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫:</b>\n\n{result}",
                        parse_mode=ParseMode.HTML
                    )
                else:
                    await wait_msg.edit_text(
                        text="‚ùó –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 20 —Å–ª–æ–≤).",
                        parse_mode=ParseMode.HTML
                    )
                    result = None

            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}. –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω.")

            await message.answer(
                "–í—ã–±–µ—Ä–∏—Ç–µ —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ:",
                reply_markup=get_main_keyboard()
            )
            logger.info(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –º–µ–Ω—é –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {user_id}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {str(e)}")
            await wait_msg.edit_text(
                text=f"<b>‚ùå –û—à–∏–±–∫–∞:</b> {str(e)}",
                parse_mode=ParseMode.HTML
            )
            await message.answer(
                "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ –∑–∞–Ω–æ–≤–æ:",
                reply_markup=get_main_keyboard()
            )

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {str(e)}")
        await message.answer(
            "‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.",
            reply_markup=get_main_keyboard()
        )
    finally:
        try:
            await state.clear()
            logger.info(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ –æ—á–∏—â–µ–Ω–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {str(e)}")


@dp.message()
async def unknown_message(message: Message):
    logger.warning(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {message.from_user.id} –æ—Ç–ø—Ä–∞–≤–∏–ª –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {message.text}")
    await message.answer(
        "‚ùó –Ø –Ω–µ –ø–æ–Ω–∏–º–∞—é —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ. –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏, –Ω–∞–∂–∞–≤ –Ω–∞ –∫–Ω–æ–ø–∫—É!",
        reply_markup=get_main_keyboard()
    )


# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
def load_models():
    logger.info("–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π...")
    global paraphraser_model, paraphraser_tokenizer, headliner_model, headliner_tokenizer

    try:
        # –ú–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è
        MODEL_PATH = "cointegrated/rut5-base-paraphraser"
        paraphraser_tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH, legacy=True)
        paraphraser_model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)

        # –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        headliner_tokenizer = AutoTokenizer.from_pretrained("IlyaGusev/rut5_base_sum_gazeta", legacy=True)
        headliner_model = AutoModelForSeq2SeqLM.from_pretrained("IlyaGusev/rut5_base_sum_gazeta").to(device)

        # –ó–∞–≥—Ä—É–∑–∫–∞ NLTK –¥–∞–Ω–Ω—ã—Ö
        nltk.download('punkt', quiet=True)
        nltk.download('punkt_tab', quiet=True)

        logger.info("–í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {str(e)}")
        raise


# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
async def main():
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...")
    try:
        load_models()
        logger.info("–ë–æ—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        await dp.start_polling(bot)
    except Exception as e:
        logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {str(e)}")
    finally:
        logger.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


if __name__ == "__main__":
    asyncio.run(main())